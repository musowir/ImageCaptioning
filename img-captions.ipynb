{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/musowir/img-captions?scriptVersionId=124020205\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Step 1:- Import the required libraries","metadata":{}},{"cell_type":"code","source":"import string\nimport numpy as np\nimport pandas as pd\nfrom numpy import array\nfrom pickle import load\n\nfrom PIL import Image\nimport pickle\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport sys, time, os, warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\n\nimport keras\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:55:00.16473Z","iopub.execute_input":"2023-03-31T04:55:00.165209Z","iopub.status.idle":"2023-03-31T04:55:10.459715Z","shell.execute_reply.started":"2023-03-31T04:55:00.165166Z","shell.execute_reply":"2023-03-31T04:55:10.458379Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Step 2:- Data loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"\nimage_path = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\ndir_Flickr_text = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n#features_path = '/kaggle/input/features/features'\nfeatures_path = '/kaggle/working/VGG19'\nif not os.path.exists(features_path):\n    os.makedirs(features_path)\njpgs = os.listdir(image_path)\n\nprint(\"Total Images in Dataset = {}\".format(len(jpgs)))","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:55:14.997415Z","iopub.execute_input":"2023-03-31T04:55:14.999206Z","iopub.status.idle":"2023-03-31T04:55:15.540593Z","shell.execute_reply.started":"2023-03-31T04:55:14.99915Z","shell.execute_reply":"2023-03-31T04:55:15.539161Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Total Images in Dataset = 31785\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We create a dataframe to store the image id and captions for ease of use.","metadata":{}},{"cell_type":"code","source":"# # file = open(dir_Flickr_text,'r')\n# # text = file.read()\n# # file.close()\n\n# # datatxt = []\n# # for line in text.split('\\n'):\n# #    col = line.split('\\t')\n# #    if len(col) == 1:\n# #        continue\n# #    w = col[0].split(\"#\")\n# #    datatxt.append(w + [col[1].lower()])\n\n# data = pd.read_csv(dir_Flickr_text, delimiter=\"|\")\n# #data = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n# data.columns = [\"filename\",\"index\",\"caption\"]\n# data = data.reindex(columns =['index','filename','caption'])\n# data.dropna(inplace=True)\n# uni_filenames = np.unique(data.filename.values)\n\n# data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"visualize a few images and their 5 captions:","metadata":{}},{"cell_type":"code","source":"# npic = 5\n# npix = 224\n# target_size = (npix,npix,3)\n# count = 1\n\n# fig = plt.figure(figsize=(10,20))\n# for jpgfnm in uni_filenames[10:14]:\n#    filename = image_path + '/' + jpgfnm\n#    captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n#    image_load = load_img(filename, target_size=target_size)\n#    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n#    ax.imshow(image_load)\n#    count += 1\n\n#    ax = fig.add_subplot(npic,2,count)\n#    plt.axis('off')\n#    ax.plot()\n#    ax.set_xlim(0,1)\n#    ax.set_ylim(0,len(captions))\n#    for i, caption in enumerate(captions):\n#        ax.text(0,i,caption,fontsize=20)\n#    count += 1\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"current vocabulary size is:-","metadata":{}},{"cell_type":"code","source":"# vocabulary = []\n# for txt in data.caption.values:\n#    vocabulary.extend(txt.split())\n# print('Vocabulary Size: %d' % len(set(vocabulary)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"some text cleaning such as removing punctuation, single characters, and numeric values:","metadata":{}},{"cell_type":"code","source":"# def remove_punctuation(text_original):\n#    text_no_punctuation = text_original.translate(string.punctuation)\n#    return(text_no_punctuation)\n\n# def remove_single_character(text):\n#    text_len_more_than1 = \"\"\n#    for word in text.split():\n#        if len(word) > 1:\n#            text_len_more_than1 += \" \" + word\n#    return(text_len_more_than1)\n\n# def remove_numeric(text):\n#    text_no_numeric = \"\"\n#    for word in text.split():\n#        isalpha = word.isalpha()\n#        if isalpha:\n#            text_no_numeric += \" \" + word\n#    return(text_no_numeric)\n\n# def text_clean(text_original):\n#    text = remove_punctuation(text_original)\n#    text = remove_single_character(text)\n#    text = remove_numeric(text)\n#    return(text)\n\n# for i, caption in enumerate(data.caption.values):\n#    newcaption = text_clean(caption)\n#    data[\"caption\"].iloc[i] = newcaption","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"size of our vocabulary after cleaning","metadata":{}},{"cell_type":"code","source":"# clean_vocabulary = []\n# for txt in data.caption.values:\n#    clean_vocabulary.extend(txt.split())\n# print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"add ‘< start >’ and ‘< end >’ tags to every caption so that the model understands the starting and end of each caption","metadata":{}},{"cell_type":"code","source":"\n# all_captions = []\n# for caption  in data[\"caption\"].astype(str):\n#    caption = '<start> ' + caption+ ' <end>'\n#    all_captions.append(caption)\n\n# all_captions[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_img_name_vector = []\n# for annot in data[\"filename\"]:\n#    full_image_path = image_path + '/' + annot\n#    all_img_name_vector.append(full_image_path)\n\n# all_img_name_vector[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"40455 image paths and captions.","metadata":{}},{"cell_type":"code","source":"# print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\n# print(f\"len(all_captions) : {len(all_captions)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"take only 40000 of each so that we can select batch size properly i.e. 625 batches if batch size= 64. To do this we define a function to limit the dataset to 40000 images and captions.","metadata":{}},{"cell_type":"code","source":"# def data_limiter(num,total_captions,all_img_name_vector):\n#  train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n#  train_captions = train_captions[:num]\n#  img_name_vector = img_name_vector[:num]\n#  return train_captions,img_name_vector\n\n# train_captions,img_name_vector = data_limiter(158720,all_captions,all_img_name_vector)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 3:- Model Definition","metadata":{}},{"cell_type":"markdown","source":"**vgg19**","metadata":{}},{"cell_type":"code","source":"def load_image(image_path):\n   img = tf.io.read_file(image_path)\n   img = tf.image.decode_jpeg(img, channels=3)\n   img = tf.image.resize(img, (224, 224))\n   img = preprocess_input(img)\n   return img, image_path\n\nimage_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n\nimage_features_extract_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:55:27.516462Z","iopub.execute_input":"2023-03-31T04:55:27.517296Z","iopub.status.idle":"2023-03-31T04:55:32.630844Z","shell.execute_reply.started":"2023-03-31T04:55:27.517253Z","shell.execute_reply":"2023-03-31T04:55:32.62941Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 4s 0us/step\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None, None, 3)]   0         \n                                                                 \n block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n                                                                 \n block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n                                                                 \n block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n                                                                 \n block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n=================================================================\nTotal params: 20,024,384\nTrainable params: 20,024,384\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Map each image name to the function to load the image:-","metadata":{}},{"cell_type":"code","source":"#to save image_features_extract_model  \n\n#image_features_extract_model.save('image_features_extract_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith open('/kaggle/input/img-caption-constants/constants/img_name_vector.pkl', 'rb') as f:\n    img_name_vector = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:55:55.941299Z","iopub.execute_input":"2023-03-31T04:55:55.942152Z","iopub.status.idle":"2023-03-31T04:55:55.989003Z","shell.execute_reply.started":"2023-03-31T04:55:55.942103Z","shell.execute_reply":"2023-03-31T04:55:55.987922Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"encode_train = sorted(set(img_name_vector))\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:55:58.567804Z","iopub.execute_input":"2023-03-31T04:55:58.568613Z","iopub.status.idle":"2023-03-31T04:55:58.873982Z","shell.execute_reply.started":"2023-03-31T04:55:58.568568Z","shell.execute_reply":"2023-03-31T04:55:58.873021Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"extract the features and store them in the respective .npy files and then pass those features through the encoder.NPY files store all the information required to reconstruct an array on any computer, which includes dtype and shape information.","metadata":{}},{"cell_type":"code","source":"%%time\nfor img, path in tqdm(image_dataset):\n batch_features = image_features_extract_model(img)\n batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n\n for bf, p in zip(batch_features, path):\n   path_of_feature = Path(p.numpy().decode(\"utf-8\"))\n   image_name = path_of_feature.stem\n   np.save(f'{features_path}/{image_name}', bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:56:03.004114Z","iopub.execute_input":"2023-03-31T04:56:03.004556Z","iopub.status.idle":"2023-03-31T05:00:25.138624Z","shell.execute_reply.started":"2023-03-31T04:56:03.004519Z","shell.execute_reply":"2023-03-31T05:00:25.137272Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"  2%|▏         | 9/497 [03:50<3:26:56, 25.44s/it]Exception ignored in: <generator object tqdm.__iter__ at 0x7fac98697c50>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/tqdm/std.py\", line 1196, in __iter__\n    yield obj\nKeyboardInterrupt: \n  2%|▏         | 9/497 [04:22<3:56:49, 29.12s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 ):\n\u001b[0;32m-> 1132\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \"\"\"\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 ):\n\u001b[0;32m-> 1132\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    281\u001b[0m             )\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36mconvolution_op\u001b[0;34m(self, inputs, kernel)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         )\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1186\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1318\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1321\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2792\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2793\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2794\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2795\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2796\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1106\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#to save the extracted features as zip file (to download)\n\n# import shutil\n# shutil.make_archive('/kaggle/working/VGG19', 'zip', '/kaggle/working/VGG19')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top_k = 5000\n# tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n#                                                  oov_token=\"<unk>\",\n#                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n\n# tokenizer.fit_on_texts(train_captions)\n# train_seqs = tokenizer.texts_to_sequences(train_captions)\n# tokenizer.word_index['<pad>'] = 0\n# tokenizer.index_word[0] = '<pad>'\n\n# train_seqs = tokenizer.texts_to_sequences(train_captions)\n# cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save the tokenizer to a file using pickle\n# with open('tokenizer.pickle', 'wb') as handle:\n#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T05:00:25.141129Z","iopub.execute_input":"2023-03-31T05:00:25.141516Z","iopub.status.idle":"2023-03-31T05:00:25.186751Z","shell.execute_reply.started":"2023-03-31T05:00:25.141475Z","shell.execute_reply":"2023-03-31T05:00:25.185054Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28/168945308.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the tokenizer to a file using pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"visualize the padded training and captions and the tokenized vectors:","metadata":{}},{"cell_type":"code","source":"# train_captions[:3]","metadata":{"execution":{"iopub.status.busy":"2023-03-31T05:00:25.190323Z","iopub.status.idle":"2023-03-31T05:00:25.191038Z","shell.execute_reply.started":"2023-03-31T05:00:25.190674Z","shell.execute_reply":"2023-03-31T05:00:25.19071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"calculate the max and min length of all captions","metadata":{}},{"cell_type":"code","source":"# def calc_max_length(tensor):\n#    return max(len(t) for t in tensor)\n# max_length = calc_max_length(train_seqs)\n\n# def calc_min_length(tensor):\n#    return min(len(t) for t in tensor)\n# min_length = calc_min_length(train_seqs)\n\n# print('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save max_length to a file using pickle\n# with open('max_length.pkl', 'wb') as f:\n#     pickle.dump(max_length, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create training and validation sets using an 80-20 split:","metadata":{}},{"cell_type":"code","source":"# img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pickle\n\n# # Save the list to a file\n# with open('img_name_train.pkl', 'wb') as f:\n#     pickle.dump(img_name_val, f)\n# # Save the list to a file\n# with open('cap_train.pkl', 'wb') as f:\n#     pickle.dump(cap_val, f)\n\n# # Save the list to a file\n# with open('img_name_val.pkl', 'wb') as f:\n#     pickle.dump(img_name_val, f)\n# # Save the list to a file\n# with open('cap_val.pkl', 'wb') as f:\n#     pickle.dump(cap_val, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the parameters for training:","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/img-caption-constants/constants/img_name_train.pkl', 'rb') as f:\n    img_name_train = pickle.load(f)\n\nwith open('/kaggle/input/img-caption-constants/constants/cap_train.pkl', 'rb') as f:\n    cap_train = pickle.load(f)\n    \nwith open('/kaggle/input/img-caption-constants/constants/img_name_val.pkl', 'rb') as f:\n    img_name_val = pickle.load(f)\n\nwith open('/kaggle/input/img-caption-constants/constants/cap_val.pkl', 'rb') as f:\n    cap_val = pickle.load(f)\n    \n# Load the tokenizer from the file\nwith open('/kaggle/input/flicker30kvgg19/tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\n# Load max_length from the saved file\nwith open('/kaggle/input/flicker30kvgg19/max_length.pkl', 'rb') as f:\n    max_length = pickle.load(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = len(tokenizer.word_index) + 1\nnum_steps = len(img_name_train) // BATCH_SIZE\nval_num_steps = len(img_name_val) // BATCH_SIZE\nfeatures_shape = 512\nattention_features_shape = 49","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a tf.data dataset to use for training our model.","metadata":{}},{"cell_type":"code","source":"import re\ndef map_func(img_name, cap):\n imgn = re.split('\\.|\\/', img_name.decode())[-2]\n img_tensor = np.load(f'{features_path}/{imgn}'+'.npy')\n return img_tensor, cap\ndataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n        map_func, [item1, item2], [tf.float32, tf.int32]),\n         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The VGG-19 Encoder is defined below:-","metadata":{}},{"cell_type":"code","source":"class VGG19_Encoder(tf.keras.Model):\n   # This encoder passes the features through a Fully connected layer\n   def __init__(self, embedding_dim):\n       super(VGG19_Encoder, self).__init__()\n       # shape after fc == (batch_size, 49, embedding_dim)\n       self.fc = tf.keras.layers.Dense(embedding_dim)\n       self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n\n   def call(self, x):\n       #x= self.dropout(x)\n       x = self.fc(x)\n       x = tf.nn.relu(x)\n       return x   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Resnet encoder is defined below:-","metadata":{}},{"cell_type":"markdown","source":"We define our RNN based on GPU/CPU capabilities-","metadata":{}},{"cell_type":"code","source":"# def rnn_type(units):\n#    if tf.test.is_gpu_available():\n#        return tf.compat.v1.keras.layers.CuDNNLSTM(units,\n#                                        return_sequences=True,\n#                                        return_state=True,\n#                                        recurrent_initializer='glorot_uniform')\n#    else:\n#        return tf.keras.layers.GRU(units,\n#                                   return_sequences=True,\n#                                   return_state=True,\n#                                   recurrent_activation='sigmoid',\n#                                   recurrent_initializer='glorot_uniform')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"define the RNN Decoder with Bahdanau Attention:","metadata":{}},{"cell_type":"code","source":"'''The encoder output(i.e. 'features'), hidden state(initialized to 0)(i.e. 'hidden') and\nthe decoder input (which is the start token)(i.e. 'x') is passed to the decoder.'''\n\nclass Rnn_Local_Decoder(tf.keras.Model):\n def __init__(self, embedding_dim, units, vocab_size):\n   super(Rnn_Local_Decoder, self).__init__()\n   self.units = units\n   self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n   self.gru = tf.keras.layers.GRU(self.units,\n                                  return_sequences=True,\n                                  return_state=True,\n                                  recurrent_initializer='glorot_uniform')\n  \n   self.fc1 = tf.keras.layers.Dense(self.units)\n\n   self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n   self.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n\n   self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n   # Implementing Attention Mechanism\n   self.Uattn = tf.keras.layers.Dense(units)\n   self.Wattn = tf.keras.layers.Dense(units)\n   self.Vattn = tf.keras.layers.Dense(1)\n\n def call(self, x, features, hidden):\n   # features shape ==> (64,49,256) ==> Output from ENCODER\n   # hidden shape == (batch_size, hidden_size) ==>(64,512)\n   # hidden_with_time_axis shape == (batch_size, 1, hidden_size) ==> (64,1,512)\n\n   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n   # score shape == (64, 49, 1)\n   # Attention Function\n   '''e(ij) = f(s(t-1),h(j))'''\n   ''' e(ij) = Vattn(T)*tanh(Uattn * h(j) + Wattn * s(t))'''\n\n   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n\n   # self.Uattn(features) : (64,49,512)\n   # self.Wattn(hidden_with_time_axis) : (64,1,512)\n   # tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)) : (64,49,512)\n   # self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis))) : (64,49,1) ==> score\n\n   # you get 1 at the last axis because you are applying score to self.Vattn\n   # Then find Probability using Softmax\n   '''attention_weights(alpha(ij)) = softmax(e(ij))'''\n\n   attention_weights = tf.nn.softmax(score, axis=1)\n\n   # attention_weights shape == (64, 49, 1)\n   # Give weights to the different pixels in the image\n   ''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n\n   context_vector = attention_weights * features\n   context_vector = tf.reduce_sum(context_vector, axis=1)\n\n   # Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n   # context_vector shape after sum == (64, 256)\n   # x shape after passing through embedding == (64, 1, 256)\n\n   x = self.embedding(x)\n   # x shape after concatenation == (64, 1,  512)\n\n   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n   # passing the concatenated vector to the GRU\n\n   output, state = self.gru(x)\n   # shape == (batch_size, max_length, hidden_size)\n\n   x = self.fc1(output)\n   # x shape == (batch_size * max_length, hidden_size)\n\n   x = tf.reshape(x, (-1, x.shape[2]))\n\n   # Adding Dropout and BatchNorm Layers\n   x= self.dropout(x)\n   x= self.batchnormalization(x)\n\n   # output shape == (64 * 512)\n   x = self.fc2(x)\n\n   # shape : (64 * 8329(vocab))\n   return x, state, attention_weights\n\n def reset_state(self, batch_size):\n   return tf.zeros((batch_size, self.units))\n\n\nencoder = VGG19_Encoder(embedding_dim)\ndecoder = Rnn_Local_Decoder(embedding_dim, units, vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the loss function and optimizers:-","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n   from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n mask = tf.math.logical_not(tf.math.equal(real, 0))\n loss_ = loss_object(real, pred)\n mask = tf.cast(mask, dtype=loss_.dtype)\n loss_ *= mask\n\n return tf.reduce_mean(loss_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 4:- Model Training","metadata":{}},{"cell_type":"code","source":"loss_plot = []\n\n@tf.function\ndef train_step(img_tensor, target):\n loss = 0\n # initializing the hidden state for each batch\n # because the captions are not related from image to image\n\n hidden = decoder.reset_state(batch_size=target.shape[0])\n dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n\n with tf.GradientTape() as tape:\n     features = encoder(img_tensor)\n     for i in range(1, target.shape[1]):\n         # passing the features through the decoder\n         predictions, hidden, _ = decoder(dec_input, features, hidden)\n         # target[:, i] -> (64,1), preds -> (64, vocab)\n         loss += loss_function(target[:, i], predictions)\n\n         # using teacher forcing\n         dec_input = tf.expand_dims(target[:, i], 1)\n\n total_loss = (loss / int(target.shape[1]))\n trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n gradients = tape.gradient(loss, trainable_variables)\n optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n return loss, total_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss_plot = []\n\n@tf.function\ndef val_step(img_tensor, target):\n loss = 0\n # initializing the hidden state for each batch\n # because the captions are not related from image to image\n\n hidden = decoder.reset_state(batch_size=target.shape[0])\n dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n\n features = encoder(img_tensor)\n for i in range(1, target.shape[1]):\n     # passing the features through the decoder\n     predictions, hidden, _ = decoder(dec_input, features, hidden)\n     # target[:, i] -> (64,1), preds -> (64, vocab)\n     loss += loss_function(target[:, i], predictions)\n\n     # using teacher forcing\n     dec_input = tf.expand_dims(target[:, i], 1)\n\n val_total_loss = (loss / int(target.shape[1]))\n#  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n#  gradients = tape.gradient(loss, trainable_variables)\n#  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n return val_total_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train the model","metadata":{}},{"cell_type":"code","source":"EPOCHS = 20\nfor epoch in range(0, EPOCHS):\n   start = time.time()\n   total_loss = 0\n   for (batch, (img_tensor, target)) in enumerate(dataset):\n       #target :(64, vcab)\n       batch_loss, t_loss = train_step(img_tensor, target)\n       total_loss += t_loss\n\n       if batch % 100 == 0:\n           print ('Epoch {} Batch {} Loss {:.4f}'.format(\n             epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n   # storing the epoch end loss value to plot later\n   loss_plot.append(total_loss / num_steps)\n\n    \n   val_total_loss = 0\n   for (batch, (img_tensor, target)) in enumerate(dataset):\n       #target :(64, vcab)\n       batch_loss, t_loss = train_step(img_tensor, target)\n       val_total_loss += t_loss\n\n    \n   # storing the epoch end loss value to plot later\n   val_loss_plot.append(total_loss / num_steps)\n\n   print ('Epoch {}: Loss {:.6f}, Val loss {:.6f} '.format(epoch + 1,\n                                        total_loss/num_steps, val_total_loss/val_num_steps))\n\n   print ('Time taken for epoch {} : {} sec\\n'.format(epoch+1, time.time() - start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot the error graph:","metadata":{}},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.plot(val_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 5:- Greedy Search and BLEU Evaluation","metadata":{}},{"cell_type":"code","source":"\n# specify the directory where you want to save the model\nmodel_dir = '/kaggle/working/decoder'\n# create the directory if it doesn't exist\nos.makedirs(model_dir, exist_ok=True)\n# save the model to the directory\ndecoder.save(model_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# shutil.make_archive('/kaggle/working/decoder', 'zip', '/kaggle/working/decoder')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = '/kaggle/working/encoder'\n# create the directory if it doesn't exist\nos.makedirs(model_dir, exist_ok=True)\n# save the model to the directory\nencoder.save(model_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shutil.make_archive('/kaggle/working/encoder', 'zip', '/kaggle/working/encoder')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"# Prediction Part","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\ndef load_image(image_path):\n   img = tf.io.read_file(image_path)\n   img = tf.image.decode_jpeg(img, channels=3)\n   img = tf.image.resize(img, (224, 224))\n   img = preprocess_input(img)\n   return img, image_path\n\n# # load the model from the saved directory\n# model_dir = '/kaggle/input/flicker30kvgg19/decoder'\n# decoder = tf.keras.models.load_model(model_dir)\n\n# model_dir = '/kaggle/input/flicker30kvgg19/encoder'\n# encoder = tf.keras.models.load_model(model_dir)\n\n# with open('/kaggle/input/flicker30kvgg19/img_name_val.pkl', 'rb') as f:\n#     img_name_val = pickle.load(f)\n\n# with open('/kaggle/input/flicker30kvgg19/cap_val.pkl', 'rb') as f:\n#     cap_val = pickle.load(f)\n# # Load the tokenizer from the file\n# with open('/kaggle/input/flicker30kvgg19/tokenizer.pickle', 'rb') as handle:\n#     tokenizer = pickle.load(handle)\n\n# # Load max_length from the saved file\n# with open('/kaggle/input/flicker30kvgg19/max_length.pkl', 'rb') as f:\n#     max_length = pickle.load(f)\n    \n# image_features_extract_model = load_model('/kaggle/input/flicker30kvgg19/image_features_extract_model.h5')\nattention_features_shape = 49","metadata":{"execution":{"iopub.status.busy":"2023-03-30T07:41:36.772774Z","iopub.execute_input":"2023-03-30T07:41:36.773326Z","iopub.status.idle":"2023-03-30T07:41:42.033061Z","shell.execute_reply.started":"2023-03-30T07:41:36.773277Z","shell.execute_reply":"2023-03-30T07:41:42.031803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define our greedy method of defining captions:\n\ndef evaluate(image):\n   attention_plot = np.zeros((max_length, attention_features_shape))\n   hidden = tf.zeros((1, 512))\n   temp_input = tf.expand_dims(load_image(image)[0], 0)\n   img_tensor_val = image_features_extract_model(temp_input)\n   \n   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n   \n   features = encoder(img_tensor_val)\n  \n   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n   result = []\n\n   for i in range(max_length):\n        \n       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n       predicted_id = tf.argmin(predictions[0]).numpy()\n       result.append(tokenizer.index_word[predicted_id])\n\n       if tokenizer.index_word[predicted_id] == '<end>':\n           return result, attention_plot\n\n       dec_input = tf.expand_dims([predicted_id], 0)\n   attention_plot = attention_plot[:len(result), :]\n\n   return result, attention_plot","metadata":{"execution":{"iopub.status.busy":"2023-03-30T07:41:32.810578Z","iopub.execute_input":"2023-03-30T07:41:32.811443Z","iopub.status.idle":"2023-03-30T07:41:32.824926Z","shell.execute_reply.started":"2023-03-30T07:41:32.811395Z","shell.execute_reply":"2023-03-30T07:41:32.823437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define a function to plot the attention maps for each word generated\n\ndef plot_attention(image, result, attention_plot):\n   temp_image = np.array(Image.open(image))\n   fig = plt.figure(figsize=(10, 10))\n   len_result = len(result)\n   for l in range(len_result):\n       temp_att = np.resize(attention_plot[l], (8, 8))\n       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n       ax.set_title(result[l])\n       img = ax.imshow(temp_image)\n       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n   plt.tight_layout()\n   plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T07:43:48.527385Z","iopub.execute_input":"2023-03-30T07:43:48.527797Z","iopub.status.idle":"2023-03-30T07:43:48.537171Z","shell.execute_reply.started":"2023-03-30T07:43:48.527764Z","shell.execute_reply":"2023-03-30T07:43:48.535842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"generate a caption for the image","metadata":{}},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nstart = time.time()\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\n# remove <start> and <end> from the real_caption\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\n#remove \"<unk>\" in result\nfor i in result:\n   if i==\"<unk>\":\n       result.remove(i)\n\nfor i in real_caption:\n   if i==\"<unk>\":\n       real_caption.remove(i)\n\n#remove <end> from result        \nresult_join = ' '.join(result)\nresult_final = result_join.rsplit(' ', 1)[0]\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = result\n\nscore = sentence_bleu(reference, candidate)\nprint(f\"BELU score: {score*100}\")\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', result_final)\n\nplot_attention(image, result, attention_plot)\nprint(f\"time took to Predict: {round(time.time()-start)} sec\")\nImage.open(img_name_val[rid])","metadata":{"execution":{"iopub.status.busy":"2023-03-30T07:43:50.764828Z","iopub.execute_input":"2023-03-30T07:43:50.766053Z","iopub.status.idle":"2023-03-30T07:43:59.031477Z","shell.execute_reply.started":"2023-03-30T07:43:50.766003Z","shell.execute_reply":"2023-03-30T07:43:59.030105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# g_cap={}\n# o_cap ={}\n# for counted in tqdm(range(len(cap_val))):\n#     rid = counted\n#     image = img_name_val[rid]\n#     real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n#     result, attention_plot = evaluate(image)\n\n#     # remove <start> and <end> from the real_caption\n#     first = real_caption.split(' ', 1)[1]\n#     real_caption = first.rsplit(' ', 1)[0]\n   \n#     #remove \"<unk>\" in result\n#     for i in result:\n#        if i==\"<unk>\":\n#            result.remove(i)\n\n#     for i in real_caption:\n#        if i==\"<unk>\":\n#            real_caption.remove(i)\n\n#     #remove <end> from result             \n#     result_join = ' '.join(result)\n#     result_final = result_join.rsplit(' ', 1)[0]\n\n    \n#     o_cap[counted]=[real_caption,]\n#     g_cap[counted]=[result_final,]\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implementing beam search","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# Define start and end tokens\nstart_token = '<start>'\nend_token = '<end>'\n\n# Define beam search function\ndef beam_search(features, decoder_model, tokenizer, beam_width=3, max_length=50):\n    # Initialize the first sequence with start token\n    hidden = tf.zeros((1, 512))\n    start_seq = tokenizer.texts_to_sequences([start_token])[0]\n    start_seq = tf.keras.preprocessing.sequence.pad_sequences([start_seq], maxlen=max_length, padding='post')\n    \n    # Initialize the beam with the first sequence and score of 1.0\n    beam = [(start_seq, 1.0)]\n    \n    # Define end token ID\n    end_id = tokenizer.word_index[end_token]\n    \n    # Iterate over the sequence\n    for i in range(max_length):\n        candidates = []\n        # Generate candidate sequences for each sequence in the beam\n        for seq, score in beam:\n            # If sequence ends with end token, add to candidates\n            if seq[0][-1] == end_id:\n                candidates.append((seq, score))\n                continue\n            # Predict the next word probabilities\n            partial_seq = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n            preds, hidden, attention_weights = decoder(partial_seq, features, hidden)\n            # Get the top-k candidate sequences\n            \n            #top_k_idx = preds.argsort()[-beam_width:]\n            top_k_idx = tf.argsort(preds[0]).numpy()\n            \n            for idx in top_k_idx:\n                candidate_seq = (np.insert(seq[0], len(seq[0]), idx),)\n                candidate_score = score * preds[0][idx]\n                candidates.append((candidate_seq, candidate_score))\n            print(candidates)\n        # Sort candidates by score and keep top-k\n        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n        # Update beam with top-k candidates\n        beam = candidates\n        \n    # Select sequence with highest score as final caption\n    final_seq = beam[0][0]\n    caption = ' '.join([tokenizer.index_word[idx] for idx in final_seq if idx not in [0, end_id]])\n    return caption\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image='/kaggle/input/flickr30k/flickr30k_images/1001573224.jpg'\nattention_plot = np.zeros((max_length, attention_features_shape))\nhidden = tf.zeros((1, 512))\ntemp_input = tf.expand_dims(load_image(image)[0], 0)\nimg_tensor_val = image_features_extract_model(temp_input)\n   \nimg_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\nimage_features = features = encoder(img_tensor_val)\n\n\nbeam_width = 3\n\ncaption = beam_search(image_features, decoder, tokenizer, beam_width, max_length)\nprint(caption)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\ndef calc_bleu(real, pred):\n    real = [[j.split() for j in i] for i in real]\n    pred = [i.split() for i in pred]\n    score = corpus_bleu(real, pred, weights=(0.25, 0.25, 0.25, 0.25))\n    return score\n\nr=[[\"I am a man\", \"I was planning a tour anyways\"], [\"Good things takes time\",\"Good and bad\"]]\np=[\"totally bad\", \"Bad things takes time\"]\nprint(calc_bleu(r, p))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:00:26.515557Z","iopub.execute_input":"2023-03-30T08:00:26.515998Z","iopub.status.idle":"2023-03-30T08:00:26.524684Z","shell.execute_reply.started":"2023-03-30T08:00:26.515958Z","shell.execute_reply":"2023-03-30T08:00:26.523653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocoevalcap ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:12:42.424547Z","iopub.execute_input":"2023-03-30T08:12:42.425893Z","iopub.status.idle":"2023-03-30T08:13:23.437061Z","shell.execute_reply.started":"2023-03-30T08:12:42.425835Z","shell.execute_reply":"2023-03-30T08:13:23.435807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\n\ndef calc_scores(ref, hypo):\n    \"\"\"\n    ref, dictionary of reference sentences (id, sentence)\n    hypo, dictionary of hypothesis sentences (id, sentence)\n    score, dictionary of scores\n    \"\"\"\n    scorers = [\n        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n        (Meteor(),\"METEOR\"),\n        (Rouge(), \"ROUGE_L\"),\n        (Cider(), \"CIDEr\")\n    ]\n    final_scores = {}\n    for scorer, method in scorers:\n        score, scores = scorer.compute_score(ref, hypo)\n        if type(score) == list:\n            for m, s in zip(method, score):\n                final_scores[m] = s\n        else:\n            final_scores[method] = score\n    return final_scores ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:24:43.571057Z","iopub.execute_input":"2023-03-30T08:24:43.571467Z","iopub.status.idle":"2023-03-30T08:24:43.580666Z","shell.execute_reply.started":"2023-03-30T08:24:43.57143Z","shell.execute_reply":"2023-03-30T08:24:43.579415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r={1:[\"how are you\", \"hi how are\"],\n  2:[\"cid samu\",\"cid musa\"]}\np={1:[\"Hi How are you\"], 2:[\"cid musa\"]}\nx = calc_scores(r, p)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:59:18.278519Z","iopub.execute_input":"2023-03-30T08:59:18.278972Z","iopub.status.idle":"2023-03-30T08:59:28.381646Z","shell.execute_reply.started":"2023-03-30T08:59:18.278926Z","shell.execute_reply":"2023-03-30T08:59:28.379774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# define the metrics and their scores\nmetrics = x.keys()\nscores = x.values()\n\n# create a bar chart\nfig, ax = plt.subplots()\nrects = ax.bar(metrics, scores)\n\n# set the title and axis labels\nax.set_title('Model Performance')\nax.set_xlabel('Metric')\nax.set_ylabel('Score')\n\n# add labels to the bars\nfor rect in rects:\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2., height, '%.3f' % height,\n            ha='center', va='bottom')\n\n# display the chart\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:59:51.669641Z","iopub.execute_input":"2023-03-30T08:59:51.670203Z","iopub.status.idle":"2023-03-30T08:59:51.911544Z","shell.execute_reply.started":"2023-03-30T08:59:51.670154Z","shell.execute_reply":"2023-03-30T08:59:51.910177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}