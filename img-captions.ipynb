{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/musowir/img-captions?scriptVersionId=124167794\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Step 1:- Import the required libraries","metadata":{}},{"cell_type":"code","source":"import string\nimport numpy as np\nimport pandas as pd\nfrom numpy import array\nfrom pickle import load\n\nfrom PIL import Image\nimport pickle\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport sys, time, os, warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\n\nimport keras\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:22.626093Z","iopub.execute_input":"2023-04-01T08:18:22.626713Z","iopub.status.idle":"2023-04-01T08:18:36.465817Z","shell.execute_reply.started":"2023-04-01T08:18:22.62666Z","shell.execute_reply":"2023-04-01T08:18:36.464182Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Step 2:- Data loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"\nimage_path = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\ndir_Flickr_text = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n#features_path = '/kaggle/input/features/features'\nfeatures_path = '/kaggle/working/VGG19'\nif not os.path.exists(features_path):\n    os.makedirs(features_path)\njpgs = os.listdir(image_path)\n\nprint(\"Total Images in Dataset = {}\".format(len(jpgs)))","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.468816Z","iopub.execute_input":"2023-04-01T08:18:36.470263Z","iopub.status.idle":"2023-04-01T08:18:36.751065Z","shell.execute_reply.started":"2023-04-01T08:18:36.470193Z","shell.execute_reply":"2023-04-01T08:18:36.749487Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Total Images in Dataset = 31785\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We create a dataframe to store the image id and captions for ease of use.","metadata":{}},{"cell_type":"code","source":"# file = open(dir_Flickr_text,'r')\n# text = file.read()\n# file.close()\n\n# datatxt = []\n# for line in text.split('\\n'):\n#    col = line.split('\\t')\n#    if len(col) == 1:\n#        continue\n#    w = col[0].split(\"#\")\n#    datatxt.append(w + [col[1].lower()])\n\n# data = pd.read_csv(dir_Flickr_text, delimiter=\"|\")\n# #data = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n# data.columns = [\"filename\",\"index\",\"caption\"]\n# data = data.reindex(columns =['index','filename','caption'])\n# data.dropna(inplace=True)\n# uni_filenames = np.unique(data.filename.values)\n\n# print(\"No of total img-cap pairs: \", len(data))\n# data.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.757376Z","iopub.execute_input":"2023-04-01T08:18:36.75846Z","iopub.status.idle":"2023-04-01T08:18:36.766464Z","shell.execute_reply.started":"2023-04-01T08:18:36.758368Z","shell.execute_reply":"2023-04-01T08:18:36.764383Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# train_data=data[:158208]\n\n# print(\"No of training data: \", len(train_data))\n# train_data.tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.772311Z","iopub.execute_input":"2023-04-01T08:18:36.773293Z","iopub.status.idle":"2023-04-01T08:18:36.781272Z","shell.execute_reply.started":"2023-04-01T08:18:36.773219Z","shell.execute_reply":"2023-04-01T08:18:36.779055Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# test_data=data[158209:]\n# print(\"No of test data: \", len(test_data))\n# test_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.784792Z","iopub.execute_input":"2023-04-01T08:18:36.786336Z","iopub.status.idle":"2023-04-01T08:18:36.793879Z","shell.execute_reply.started":"2023-04-01T08:18:36.786271Z","shell.execute_reply":"2023-04-01T08:18:36.791489Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"visualize a few images and their 5 captions:","metadata":{}},{"cell_type":"code","source":"# npic = 5\n# npix = 224\n# target_size = (npix,npix,3)\n# count = 1\n\n# fig = plt.figure(figsize=(10,20))\n# for jpgfnm in uni_filenames[10:14]:\n#    filename = image_path + '/' + jpgfnm\n#    captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n#    image_load = load_img(filename, target_size=target_size)\n#    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n#    ax.imshow(image_load)\n#    count += 1\n\n#    ax = fig.add_subplot(npic,2,count)\n#    plt.axis('off')\n#    ax.plot()\n#    ax.set_xlim(0,1)\n#    ax.set_ylim(0,len(captions))\n#    for i, caption in enumerate(captions):\n#        ax.text(0,i,caption,fontsize=20)\n#    count += 1\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.797052Z","iopub.execute_input":"2023-04-01T08:18:36.79856Z","iopub.status.idle":"2023-04-01T08:18:36.80807Z","shell.execute_reply.started":"2023-04-01T08:18:36.798224Z","shell.execute_reply":"2023-04-01T08:18:36.806879Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"current vocabulary size is:-","metadata":{}},{"cell_type":"code","source":"# vocabulary = []\n# for txt in train_data.caption.values:\n#    vocabulary.extend(txt.split())\n# print('Vocabulary Size: %d' % len(set(vocabulary)))","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.810857Z","iopub.execute_input":"2023-04-01T08:18:36.81217Z","iopub.status.idle":"2023-04-01T08:18:36.821989Z","shell.execute_reply.started":"2023-04-01T08:18:36.812107Z","shell.execute_reply":"2023-04-01T08:18:36.820647Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"some text cleaning such as removing punctuation, single characters, and numeric values:","metadata":{}},{"cell_type":"code","source":"# def remove_punctuation(text_original):\n#    text_no_punctuation = text_original.translate(string.punctuation)\n#    return(text_no_punctuation)\n\n# def remove_single_character(text):\n#    text_len_more_than1 = \"\"\n#    for word in text.split():\n#        if len(word) > 1:\n#            text_len_more_than1 += \" \" + word\n#    return(text_len_more_than1)\n\n# def remove_numeric(text):\n#    text_no_numeric = \"\"\n#    for word in text.split():\n#        isalpha = word.isalpha()\n#        if isalpha:\n#            text_no_numeric += \" \" + word\n#    return(text_no_numeric)\n\n# def text_clean(text_original):\n#    text = remove_punctuation(text_original)\n#    text = remove_single_character(text)\n#    text = remove_numeric(text)\n#    return(text)\n\n# for i, caption in enumerate(train_data.caption.values):\n#    newcaption = text_clean(caption)\n#    train_data[\"caption\"].iloc[i] = newcaption\n    \n# for i, caption in enumerate(test_data.caption.values):\n#    newcaption = text_clean(caption)\n#    test_data[\"caption\"].iloc[i] = newcaption","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.824415Z","iopub.execute_input":"2023-04-01T08:18:36.825809Z","iopub.status.idle":"2023-04-01T08:18:36.836158Z","shell.execute_reply.started":"2023-04-01T08:18:36.825671Z","shell.execute_reply":"2023-04-01T08:18:36.834527Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"size of our vocabulary after cleaning","metadata":{}},{"cell_type":"code","source":"# clean_vocabulary = []\n# for txt in train_data.caption.values:\n#    clean_vocabulary.extend(txt.split())\n# print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.838462Z","iopub.execute_input":"2023-04-01T08:18:36.839403Z","iopub.status.idle":"2023-04-01T08:18:36.852849Z","shell.execute_reply.started":"2023-04-01T08:18:36.839341Z","shell.execute_reply":"2023-04-01T08:18:36.851538Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"add ‘< start >’ and ‘< end >’ tags to every caption so that the model understands the starting and end of each caption","metadata":{}},{"cell_type":"code","source":"\n# train_captions = []\n# for caption  in train_data[\"caption\"].astype(str):\n#    caption = '<start> ' + caption+ ' <end>'\n#    train_captions.append(caption)\n\n# train_captions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.86162Z","iopub.execute_input":"2023-04-01T08:18:36.862636Z","iopub.status.idle":"2023-04-01T08:18:36.868792Z","shell.execute_reply.started":"2023-04-01T08:18:36.862588Z","shell.execute_reply":"2023-04-01T08:18:36.867187Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# img_name_vector = []\n# for annot in train_data[\"filename\"]:\n#    full_image_path = image_path + '/' + annot\n#    img_name_vector.append(full_image_path)\n\n# img_name_vector[:10]","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.870701Z","iopub.execute_input":"2023-04-01T08:18:36.872387Z","iopub.status.idle":"2023-04-01T08:18:36.881723Z","shell.execute_reply.started":"2023-04-01T08:18:36.872325Z","shell.execute_reply":"2023-04-01T08:18:36.879873Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# import pickle\n\n# # Save the list to a file\n# with open('train_captions.pkl', 'wb') as f:\n#     pickle.dump(train_captions, f)\n# # Save the list to a file\n# with open('img_name_vector.pkl', 'wb') as f:\n#     pickle.dump(img_name_vector, f)\n\n# # Save the list to a file\n# with open('test_data.pkl', 'wb') as f:\n#     pickle.dump(test_data, f)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.884062Z","iopub.execute_input":"2023-04-01T08:18:36.886008Z","iopub.status.idle":"2023-04-01T08:18:36.89337Z","shell.execute_reply.started":"2023-04-01T08:18:36.885946Z","shell.execute_reply":"2023-04-01T08:18:36.891393Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/img-cap-consts/img_name_vector.pkl', 'rb') as f:\n    img_name_vector = pickle.load(f)\n    \nwith open('/kaggle/input/img-cap-consts/train_captions.pkl', 'rb') as f:\n    train_captions = pickle.load(f)\n    \nwith open('/kaggle/input/img-cap-consts/test_data.pkl', 'rb') as f:\n    test_data = pickle.load(f)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:36.895615Z","iopub.execute_input":"2023-04-01T08:18:36.897121Z","iopub.status.idle":"2023-04-01T08:18:37.277332Z","shell.execute_reply.started":"2023-04-01T08:18:36.897057Z","shell.execute_reply":"2023-04-01T08:18:37.275556Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"40455 image paths and captions.","metadata":{}},{"cell_type":"code","source":"print(f\"len(all_img_name_vector) : {len(img_name_vector)}\")\nprint(f\"len(all_captions) : {len(train_captions)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:37.279336Z","iopub.execute_input":"2023-04-01T08:18:37.279969Z","iopub.status.idle":"2023-04-01T08:18:37.302509Z","shell.execute_reply.started":"2023-04-01T08:18:37.279909Z","shell.execute_reply":"2023-04-01T08:18:37.300817Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"len(all_img_name_vector) : 158208\nlen(all_captions) : 158208\n","output_type":"stream"}]},{"cell_type":"markdown","source":"take only 40000 of each so that we can select batch size properly i.e. 625 batches if batch size= 64. To do this we define a function to limit the dataset to 40000 images and captions.","metadata":{}},{"cell_type":"code","source":"# def data_limiter(num,total_captions,all_img_name_vector):\n#  train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n#  train_captions = train_captions[:num]\n#  img_name_vector = img_name_vector[:num]\n#  return train_captions,img_name_vector\n\n# train_captions,img_name_vector = data_limiter(158720,all_captions,all_img_name_vector)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:37.305719Z","iopub.execute_input":"2023-04-01T08:18:37.30699Z","iopub.status.idle":"2023-04-01T08:18:37.318743Z","shell.execute_reply.started":"2023-04-01T08:18:37.306926Z","shell.execute_reply":"2023-04-01T08:18:37.316576Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Step 3:- Model Definition","metadata":{}},{"cell_type":"markdown","source":"**vgg19**","metadata":{}},{"cell_type":"code","source":"def load_image(image_path):\n   img = tf.io.read_file(image_path)\n   img = tf.image.decode_jpeg(img, channels=3)\n   img = tf.image.resize(img, (224, 224))\n   img = preprocess_input(img)\n   return img, image_path\n\nimage_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n\nimage_features_extract_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:37.321468Z","iopub.execute_input":"2023-04-01T08:18:37.322919Z","iopub.status.idle":"2023-04-01T08:18:45.89085Z","shell.execute_reply.started":"2023-04-01T08:18:37.322771Z","shell.execute_reply":"2023-04-01T08:18:45.889715Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 4s 0us/step\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None, None, 3)]   0         \n                                                                 \n block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n                                                                 \n block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n                                                                 \n block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n                                                                 \n block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n=================================================================\nTotal params: 20,024,384\nTrainable params: 20,024,384\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Map each image name to the function to load the image:-","metadata":{}},{"cell_type":"code","source":"#to save image_features_extract_model  \n\n#image_features_extract_model.save('image_features_extract_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:45.8923Z","iopub.execute_input":"2023-04-01T08:18:45.893029Z","iopub.status.idle":"2023-04-01T08:18:45.899282Z","shell.execute_reply.started":"2023-04-01T08:18:45.892989Z","shell.execute_reply":"2023-04-01T08:18:45.89801Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"encode_train = sorted(set(img_name_vector))\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:45.901099Z","iopub.execute_input":"2023-04-01T08:18:45.901792Z","iopub.status.idle":"2023-04-01T08:18:46.235325Z","shell.execute_reply.started":"2023-04-01T08:18:45.901736Z","shell.execute_reply":"2023-04-01T08:18:46.233879Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"extract the features and store them in the respective .npy files and then pass those features through the encoder.NPY files store all the information required to reconstruct an array on any computer, which includes dtype and shape information.","metadata":{}},{"cell_type":"code","source":"%%time\nfor img, path in tqdm(image_dataset):\n batch_features = image_features_extract_model(img)\n batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n\n for bf, p in zip(batch_features, path):\n   path_of_feature = Path(p.numpy().decode(\"utf-8\"))\n   image_name = path_of_feature.stem\n   np.save(f'{features_path}/{image_name}', bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-04-01T08:18:46.237822Z","iopub.execute_input":"2023-04-01T08:18:46.238858Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 31%|███▏      | 155/495 [01:37<03:18,  1.71it/s]","output_type":"stream"}]},{"cell_type":"code","source":"#to save the extracted features as zip file (to download)\n\n# import shutil\n# shutil.make_archive('/kaggle/working/VGG19', 'zip', '/kaggle/working/VGG19')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                 oov_token=\"<unk>\",\n                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the tokenizer to a file using pickle\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('cap_vector.pkl', 'wb') as f:\n    pickle.dump(cap_vector, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/img-cap-consts/tokenizer.pickle', 'rb') as f:\n    tokenizer = pickle.load(f)\n\nwith open('/kaggle/input/img-cap-consts/cap_vector.pkl', 'rb') as f:\n    cap_vector = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"visualize the padded training and captions and the tokenized vectors:","metadata":{}},{"cell_type":"code","source":"train_captions[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"calculate the max and min length of all captions","metadata":{}},{"cell_type":"code","source":"# def calc_max_length(tensor):\n#    return max(len(t) for t in tensor)\n# max_length = calc_max_length(train_seqs)\n\n# def calc_min_length(tensor):\n#    return min(len(t) for t in tensor)\n# min_length = calc_min_length(train_seqs)\n\n# print('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save max_length to a file using pickle\n# with open('max_length.pkl', 'wb') as f:\n#     pickle.dump(max_length, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/img-cap-consts/max_length.pkl', 'rb') as f:\n    max_length = pickle.load(f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create training and validation sets using an 80-20 split:","metadata":{}},{"cell_type":"code","source":"# img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the parameters for training:","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = len(tokenizer.word_index) + 1\nnum_steps = len(img_name_vector) // BATCH_SIZE\nfeatures_shape = 512\nattention_features_shape = 49","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a tf.data dataset to use for training our model.","metadata":{}},{"cell_type":"code","source":"import re\ndef map_func(img_name, cap):\n imgn = re.split('\\.|\\/', img_name.decode())[-2]\n img_tensor = np.load(f'{features_path}/{imgn}'+'.npy')\n return img_tensor, cap\ndataset = tf.data.Dataset.from_tensor_slices((img_name_vector, cap_vector))\n\n# Use map to load the numpy files in parallel\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n        map_func, [item1, item2], [tf.float32, tf.int32]),\n         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The VGG-19 Encoder is defined below:-","metadata":{}},{"cell_type":"code","source":"class VGG19_Encoder(tf.keras.Model):\n   # This encoder passes the features through a Fully connected layer\n   def __init__(self, embedding_dim):\n       super(VGG19_Encoder, self).__init__()\n       # shape after fc == (batch_size, 49, embedding_dim)\n       self.fc = tf.keras.layers.Dense(embedding_dim)\n       self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n\n   def call(self, x):\n       #x= self.dropout(x)\n       x = self.fc(x)\n       x = tf.nn.relu(x)\n       return x   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Resnet encoder is defined below:-","metadata":{}},{"cell_type":"markdown","source":"We define our RNN based on GPU/CPU capabilities-","metadata":{}},{"cell_type":"code","source":"# def rnn_type(units):\n#    if tf.test.is_gpu_available():\n#        return tf.compat.v1.keras.layers.CuDNNLSTM(units,\n#                                        return_sequences=True,\n#                                        return_state=True,\n#                                        recurrent_initializer='glorot_uniform')\n#    else:\n#        return tf.keras.layers.GRU(units,\n#                                   return_sequences=True,\n#                                   return_state=True,\n#                                   recurrent_activation='sigmoid',\n#                                   recurrent_initializer='glorot_uniform')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"define the RNN Decoder with Bahdanau Attention:","metadata":{}},{"cell_type":"code","source":"'''The encoder output(i.e. 'features'), hidden state(initialized to 0)(i.e. 'hidden') and\nthe decoder input (which is the start token)(i.e. 'x') is passed to the decoder.'''\n\nclass Rnn_Local_Decoder(tf.keras.Model):\n def __init__(self, embedding_dim, units, vocab_size):\n   super(Rnn_Local_Decoder, self).__init__()\n   self.units = units\n   self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n   self.gru = tf.keras.layers.GRU(self.units,\n                                  return_sequences=True,\n                                  return_state=True,\n                                  recurrent_initializer='glorot_uniform')\n  \n   self.fc1 = tf.keras.layers.Dense(self.units)\n\n   self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n   self.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n\n   self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n   # Implementing Attention Mechanism\n   self.Uattn = tf.keras.layers.Dense(units)\n   self.Wattn = tf.keras.layers.Dense(units)\n   self.Vattn = tf.keras.layers.Dense(1)\n\n def call(self, x, features, hidden):\n   # features shape ==> (64,49,256) ==> Output from ENCODER\n   # hidden shape == (batch_size, hidden_size) ==>(64,512)\n   # hidden_with_time_axis shape == (batch_size, 1, hidden_size) ==> (64,1,512)\n\n   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n   # score shape == (64, 49, 1)\n   # Attention Function\n   '''e(ij) = f(s(t-1),h(j))'''\n   ''' e(ij) = Vattn(T)*tanh(Uattn * h(j) + Wattn * s(t))'''\n\n   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n\n   # self.Uattn(features) : (64,49,512)\n   # self.Wattn(hidden_with_time_axis) : (64,1,512)\n   # tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)) : (64,49,512)\n   # self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis))) : (64,49,1) ==> score\n\n   # you get 1 at the last axis because you are applying score to self.Vattn\n   # Then find Probability using Softmax\n   '''attention_weights(alpha(ij)) = softmax(e(ij))'''\n\n   attention_weights = tf.nn.softmax(score, axis=1)\n\n   # attention_weights shape == (64, 49, 1)\n   # Give weights to the different pixels in the image\n   ''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n\n   context_vector = attention_weights * features\n   context_vector = tf.reduce_sum(context_vector, axis=1)\n\n   # Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n   # context_vector shape after sum == (64, 256)\n   # x shape after passing through embedding == (64, 1, 256)\n\n   x = self.embedding(x)\n   # x shape after concatenation == (64, 1,  512)\n\n   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n   # passing the concatenated vector to the GRU\n\n   output, state = self.gru(x)\n   # shape == (batch_size, max_length, hidden_size)\n\n   x = self.fc1(output)\n   # x shape == (batch_size * max_length, hidden_size)\n\n   x = tf.reshape(x, (-1, x.shape[2]))\n\n   # Adding Dropout and BatchNorm Layers\n   x= self.dropout(x)\n   x= self.batchnormalization(x)\n\n   # output shape == (64 * 512)\n   x = self.fc2(x)\n\n   # shape : (64 * 8329(vocab))\n   return x, state, attention_weights\n\n def reset_state(self, batch_size):\n   return tf.zeros((batch_size, self.units))\n\n\nencoder = VGG19_Encoder(embedding_dim)\ndecoder = Rnn_Local_Decoder(embedding_dim, units, vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the loss function and optimizers:-","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n   from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n mask = tf.math.logical_not(tf.math.equal(real, 0))\n loss_ = loss_object(real, pred)\n mask = tf.cast(mask, dtype=loss_.dtype)\n loss_ *= mask\n\n return tf.reduce_mean(loss_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 4:- Model Training","metadata":{}},{"cell_type":"code","source":"loss_plot = []\n\n@tf.function\ndef train_step(img_tensor, target):\n loss = 0\n # initializing the hidden state for each batch\n # because the captions are not related from image to image\n\n hidden = decoder.reset_state(batch_size=target.shape[0])\n dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n\n with tf.GradientTape() as tape:\n     features = encoder(img_tensor)\n     for i in range(1, target.shape[1]):\n         # passing the features through the decoder\n         predictions, hidden, _ = decoder(dec_input, features, hidden)\n         # target[:, i] -> (64,1), preds -> (64, vocab)\n         loss += loss_function(target[:, i], predictions)\n\n         # using teacher forcing\n         dec_input = tf.expand_dims(target[:, i], 1)\n\n total_loss = (loss / int(target.shape[1]))\n trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n gradients = tape.gradient(loss, trainable_variables)\n optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n return loss, total_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train the model","metadata":{}},{"cell_type":"code","source":"EPOCHS = 5\nfor epoch in range(0, EPOCHS):\n   start = time.time()\n   total_loss = 0\n   for (batch, (img_tensor, target)) in enumerate(dataset):\n       #target :(64, vcab)\n       batch_loss, t_loss = train_step(img_tensor, target)\n       total_loss += t_loss\n\n       if batch % 100 == 0:\n           print ('Epoch {} Batch {} Loss {:.4f}'.format(\n             epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n   # storing the epoch end loss value to plot later\n   loss_plot.append(total_loss / num_steps)\n\n\n   print ('Epoch {}: Loss {:.6f}'.format(epoch + 1,\n                                        total_loss/num_steps))\n\n   print ('Time taken for epoch {} : {} sec\\n'.format(epoch +1, time.time() - start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot the error graph:","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 5:- Greedy Search and BLEU Evaluation","metadata":{}},{"cell_type":"code","source":"\n# # specify the directory where you want to save the model\n# model_dir = '/kaggle/working/decoder'\n# # create the directory if it doesn't exist\n# os.makedirs(model_dir, exist_ok=True)\n# # save the model to the directory\n# decoder.save(model_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# shutil.make_archive('/kaggle/working/decoder', 'zip', '/kaggle/working/decoder')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_dir = '/kaggle/working/encoder'\n# # create the directory if it doesn't exist\n# os.makedirs(model_dir, exist_ok=True)\n# # save the model to the directory\n# encoder.save(model_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shutil.make_archive('/kaggle/working/encoder', 'zip', '/kaggle/working/encoder')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"# Prediction Part","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/img-cap-consts/test_data.pkl', 'rb') as f:\n    test_data = pickle.load(f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef load_image(image_path):\n   img = tf.io.read_file(image_path)\n   img = tf.image.decode_jpeg(img, channels=3)\n   img = tf.image.resize(img, (224, 224))\n   img = preprocess_input(img)\n   return img, image_path\n\n# # load the model from the saved directory\n# model_dir = '/kaggle/input/flicker30kvgg19/decoder'\n# decoder = tf.keras.models.load_model(model_dir)\n\n# model_dir = '/kaggle/input/flicker30kvgg19/encoder'\n# encoder = tf.keras.models.load_model(model_dir)\n\n# with open('/kaggle/input/flicker30kvgg19/img_name_val.pkl', 'rb') as f:\n#     img_name_val = pickle.load(f)\n\n# with open('/kaggle/input/flicker30kvgg19/cap_val.pkl', 'rb') as f:\n#     cap_val = pickle.load(f)\n# # Load the tokenizer from the file\n# with open('/kaggle/input/flicker30kvgg19/tokenizer.pickle', 'rb') as handle:\n#     tokenizer = pickle.load(handle)\n\n# # Load max_length from the saved file\n# with open('/kaggle/input/flicker30kvgg19/max_length.pkl', 'rb') as f:\n#     max_length = pickle.load(f)\n    \n# image_features_extract_model = load_model('/kaggle/input/flicker30kvgg19/image_features_extract_model.h5')\nattention_features_shape = 49","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define our greedy method of defining captions:\n\ndef evaluate(image):\n   attention_plot = np.zeros((max_length, attention_features_shape))\n   hidden = tf.zeros((1, 512))\n   temp_input = tf.expand_dims(load_image(image)[0], 0)\n   img_tensor_val = image_features_extract_model(temp_input)\n   \n   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n   \n   features = encoder(img_tensor_val)\n  \n   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n   result = []\n\n   for i in range(max_length):\n        \n       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n       predicted_id = tf.argmax(predictions[0]).numpy()\n       result.append(tokenizer.index_word[predicted_id])\n\n       if tokenizer.index_word[predicted_id] == '<end>':\n           return result, attention_plot\n\n       dec_input = tf.expand_dims([predicted_id], 0)\n   attention_plot = attention_plot[:len(result), :]\n\n   return result, attention_plot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define a function to plot the attention maps for each word generated\n\ndef plot_attention(image, result, attention_plot):\n   temp_image = np.array(Image.open(image))\n   fig = plt.figure(figsize=(10, 10))\n   len_result = len(result)\n   for l in range(len_result):\n       temp_att = np.resize(attention_plot[l], (8, 8))\n       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n       ax.set_title(result[l])\n       img = ax.imshow(temp_image)\n       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n   plt.tight_layout()\n   plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"generate a caption for the image","metadata":{}},{"cell_type":"code","source":"# rid = np.random.randint(0, len(img_name_val))\n# image = img_name_val[rid]\n# start = time.time()\n# real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n# result, attention_plot = evaluate(image)\n\n# # remove <start> and <end> from the real_caption\n# first = real_caption.split(' ', 1)[1]\n# real_caption = first.rsplit(' ', 1)[0]\n\n# #remove \"<unk>\" in result\n# for i in result:\n#    if i==\"<unk>\":\n#        result.remove(i)\n\n# for i in real_caption:\n#    if i==\"<unk>\":\n#        real_caption.remove(i)\n\n# #remove <end> from result        \n# result_join = ' '.join(result)\n# result_final = result_join.rsplit(' ', 1)[0]\n\n# real_appn = []\n# real_appn.append(real_caption.split())\n# reference = real_appn\n# candidate = result\n\n# score = sentence_bleu(reference, candidate)\n# print(f\"BELU score: {score*100}\")\n\n# print ('Real Caption:', real_caption)\n# print ('Prediction Caption:', result_final)\n\n# plot_attention(image, result, attention_plot)\n# print(f\"time took to Predict: {round(time.time()-start)} sec\")\n# Image.open(img_name_val[rid])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"u_names = test_data.filename.unique()\nimage_path = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\nreference={}\npredicted_caps = {}\nfor i in tqdm(u_names):\n    reference[i] = test_data.loc[test_data['filename'] == i]['caption'].to_list()\n    img = image_path + \"/\" + i\n    result, attention_plot = evaluate(img)\n    #remove \"<unk>\" in result\n    for j in result:\n       if j==\"<unk>\":\n            result.remove(j)\n    result_join = ' '.join(result)\n    result_final = result_join.rsplit(' ', 1)[0]\n    predicted_caps[i] = [result_final] \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating Scores","metadata":{}},{"cell_type":"code","source":"!pip install pycocoevalcap ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\n\ndef calc_scores(ref, hypo):\n    \"\"\"\n    ref, dictionary of reference sentences (id, sentence)\n    hypo, dictionary of hypothesis sentences (id, sentence)\n    score, dictionary of scores\n    \"\"\"\n    scorers = [\n        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n        (Meteor(),\"METEOR\"),\n        (Rouge(), \"ROUGE_L\"),\n        (Cider(), \"CIDEr\")\n    ]\n    final_scores = {}\n    for scorer, method in scorers:\n        score, scores = scorer.compute_score(ref, hypo)\n        if type(score) == list:\n            for m, s in zip(method, score):\n                final_scores[m] = s\n        else:\n            final_scores[method] = score\n    return final_scores ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = calc_scores(reference, predicted_caps)\nprint(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# define the metrics and their scores\nmetrics = x.keys()\nscores = x.values()\n\n# create a bar chart\nfig, ax = plt.subplots()\nrects = ax.bar(metrics, scores)\n\n# set the title and axis labels\nax.set_title('Model Performance')\nax.set_xlabel('Metric')\nax.set_ylabel('Score')\n\n# add labels to the bars\nfor rect in rects:\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2., height, '%.3f' % height,\n            ha='center', va='bottom')\n\n# display the chart\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implementing beam search","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# import numpy as np\n\n# # Define start and end tokens\n# start_token = '<start>'\n# end_token = '<end>'\n\n# # Define beam search function\n# def beam_search(features, decoder_model, tokenizer, beam_width=3, max_length=50):\n#     # Initialize the first sequence with start token\n#     hidden = tf.zeros((1, 512))\n#     start_seq = tokenizer.texts_to_sequences([start_token])[0]\n#     start_seq = tf.keras.preprocessing.sequence.pad_sequences([start_seq], maxlen=max_length, padding='post')\n    \n#     # Initialize the beam with the first sequence and score of 1.0\n#     beam = [(start_seq, 1.0)]\n    \n#     # Define end token ID\n#     end_id = tokenizer.word_index[end_token]\n    \n#     # Iterate over the sequence\n#     for i in range(max_length):\n#         candidates = []\n#         # Generate candidate sequences for each sequence in the beam\n#         for seq, score in beam:\n#             # If sequence ends with end token, add to candidates\n#             if seq[0][-1] == end_id:\n#                 candidates.append((seq, score))\n#                 continue\n#             # Predict the next word probabilities\n#             partial_seq = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n#             preds, hidden, attention_weights = decoder(partial_seq, features, hidden)\n#             # Get the top-k candidate sequences\n            \n#             #top_k_idx = preds.argsort()[-beam_width:]\n#             top_k_idx = tf.argsort(preds[0]).numpy()\n            \n#             for idx in top_k_idx:\n#                 candidate_seq = (np.insert(seq[0], len(seq[0]), idx),)\n#                 candidate_score = score * preds[0][idx]\n#                 candidates.append((candidate_seq, candidate_score))\n#             print(candidates)\n#         # Sort candidates by score and keep top-k\n#         candidates = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n#         # Update beam with top-k candidates\n#         beam = candidates\n        \n#     # Select sequence with highest score as final caption\n#     final_seq = beam[0][0]\n#     caption = ' '.join([tokenizer.index_word[idx] for idx in final_seq if idx not in [0, end_id]])\n#     return caption\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image='/kaggle/input/flickr30k/flickr30k_images/1001573224.jpg'\n# attention_plot = np.zeros((max_length, attention_features_shape))\n# hidden = tf.zeros((1, 512))\n# temp_input = tf.expand_dims(load_image(image)[0], 0)\n# img_tensor_val = image_features_extract_model(temp_input)\n   \n# img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n# image_features = features = encoder(img_tensor_val)\n\n\n# beam_width = 3\n\n# caption = beam_search(image_features, decoder, tokenizer, beam_width, max_length)\n# print(caption)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}